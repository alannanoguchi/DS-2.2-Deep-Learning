{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dive Deeper into Classification Loss Functions\n",
    "\n",
    "- For two-class classification problem the last layer activation function is sigmoid and loss function would be binary cross entropy\n",
    "\n",
    "- For multi-class classification problem the last layer activation function is softmax and loss function would be cross entropy\n",
    "\n",
    "- Lets learn more about binary cross entropy and cross entropy function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets review sigmoid function and softmax function\n",
    "\n",
    "- Read the following blog post for 7 minutes\n",
    "\n",
    "https://www.depends-on-the-definition.com/guide-to-multi-label-classification-with-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Numpy\n",
    "\n",
    "- For each data sample (for each image as an example), when we have two-class classification problem then `y_true` and `y_pred` would be scalar\n",
    "\n",
    "- For a couple of data samples (images) then `y_true` and `y_pred` are arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def binary_cross_entropy_loss(y_pred, y):\n",
    "    return (-y * np.log(y_pred) - (1 - y) * np.log(1 - y_pred)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051565782628\n"
     ]
    }
   ],
   "source": [
    "y_true = 1     # y_true is the label\n",
    "y_pred = 0.9\n",
    "# y_pred = 0.2     \n",
    "\n",
    "# when y predict is closer to the true label, the binary cross entropy loss is less\n",
    "print(binary_cross_entropy_loss(y_pred, y_true))   # with y_pred as 0.9 = 0.105 vs y_pred as 0.2 = 1.609\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the y_pred is not good (not close enough to y_true), then the loss for Binary Cross Entropy is larger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y_t = 1    # true label\n",
    "y_p = 0.9  # predictive label\n",
    "# y_p = 0.2\n",
    "\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.convert_to_tensor(y_t, dtype=tf.float32),\n",
    "\n",
    "                                               logits=tf.convert_to_tensor(np.log(y_p/(1-y_p)), dtype=tf.float32))\n",
    "\n",
    "binary_cross_entropy = tf.reduce_mean(cost)   # calculates mean with only 1 image, use if there are manny images\n",
    "sess = tf.Session()\n",
    "\n",
    "print(sess.run(binary_cross_entropy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Cross Entropy in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536054\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def f_k_binary_cross_entropy(y_tr, y_pr):\n",
    "    return K.mean(K.binary_crossentropy(y_tr, y_pr), axis=-1)\n",
    "\n",
    "y_t = [1]    # use a list[]\n",
    "y_p = [0.9]\n",
    "\n",
    "print(K.eval(f_k_binary_cross_entropy(K.constant(y_t), K.constant(y_p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review of Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.4689955935892812\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-p[i]*np.log2(p[i]) for i in range(len(p))]).sum()\n",
    "    return H\n",
    "    \n",
    "p = [.5, .5]\n",
    "print(entropy(p))\n",
    "\n",
    "p = [.9, .1]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What if:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: divide by zero encountered in log2\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/miladtoutounchian/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "p= [1, 0]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - We get nan because np.log(0) is undefined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So we should modify our entropy implementation so that it can take care of the value 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = 1e-6   # here is the modification\n",
    "\n",
    "def entropy(p):\n",
    "    H = np.array([-p[i]*np.log2(np.clip(p[i], eps, 1-eps)) for i in range(len(p))]).sum()    # more modification\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4426957622784505e-06\n"
     ]
    }
   ],
   "source": [
    "p= [1, 0]\n",
    "print(entropy(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy\n",
    "\n",
    "- For each data sample (each image as an example), when we have multi-class classification problem then `y_true` and `y_pred` would be a vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.916290731874155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "eps = 1e-6\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "\t#return -sum([p[i]*np.log(q[i]) for i in range(len(p))])\n",
    "#     print([np.clip(q[i], eps, 1-eps) for i in range(len(p))])\n",
    "    return -sum([p[i]*np.log(np.clip(q[i], eps, 1-eps)) for i in range(len(p))])\n",
    "\n",
    "y_t = np.array([1, 0, 0, 0, 0])    # the true label with 5 classes\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])    # the model output. 0.4 is the probability class 1\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])  # we expect the cross entropy to be lower because it is closer to the y_true value\n",
    "print(cross_entropy(y_t, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "\n",
    "Imagine the above `y_t` and `y_p` are given: Is cross_entropy(y_t, y_p) the same as cross_entropy(y_p, y_t)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.289306734778764\n"
     ]
    }
   ],
   "source": [
    "print(cross_entropy(y_p, y_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Entropy measures the similarities between two probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162907318741551\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import entropy\n",
    "\n",
    "def cross_entropy_via_scipy(x, y):\n",
    "        ''' SEE: https://en.wikipedia.org/wiki/Cross_entropy'''\n",
    "        return  entropy(x, y)\n",
    "    \n",
    "print(cross_entropy_via_scipy(y_t, y_p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def keras_categorical_crossentropy(y_true, y_pred):\n",
    "    return K.categorical_crossentropy(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162909\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8\n",
    "# The data are from the above link\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])  \n",
    "\n",
    "print(K.eval(keras_categorical_crossentropy(K.constant(y_t), K.constant(y_p))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy in Tensorflow (low level implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9162907\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/tensorflow/tensorflow/issues/2462\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "\n",
    "y_pred_tf = tf.convert_to_tensor(y_p, np.float32)\n",
    "y_true_tf = tf.convert_to_tensor(y_t, np.float32)\n",
    "eps = 1e-6\n",
    "cliped_y_pred_tf = tf.clip_by_value(y_pred_tf, eps, 1-eps)\n",
    "loss_tf = tf.reduce_mean(-tf.reduce_sum(y_true_tf * tf.log(cliped_y_pred_tf)))\n",
    "with tf.Session() as sess:\n",
    "    loss = sess.run(loss_tf)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another implementation of Cross Entropy in Tensorflow (highe level implemenation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91629076\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "y_t = np.array([1, 0, 0, 0, 0])\n",
    "y_p = np.array([0.4, 0.3, 0.05, 0.05, 0.2])\n",
    "# y_p = np.array([0.98, 0.01, 0, 0, 0.01])\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(onehot_labels=tf.convert_to_tensor(y_t, dtype=tf.float32), logits=tf.convert_to_tensor(np.log(y_p), dtype=tf.float32))\n",
    "sess = tf.Session()\n",
    "print(sess.run(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: How we can use Deep Learning (CNN for example) for sound classification?\n",
    "\n",
    "- What is Spectogram: https://towardsdatascience.com/understanding-audio-data-fourier-transform-fft-spectrogram-and-speech-recognition-a4072d228520 \n",
    "\n",
    "- https://medium.com/x8-the-ai-community/audio-classification-using-cnn-coding-example-f9cbd272269e\n",
    "\n",
    "- https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8605515"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource\n",
    "\n",
    "- https://machinelearningmastery.com/cross-entropy-for-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H(P) = –\\sum_{x \\in X} p(x) log(p(x))$\n",
    "\n",
    "$H(P, Q) = H(P) + KL(P || Q)$\n",
    "\n",
    "$H(P, Q) != H(Q, P)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log2\n",
    " \n",
    "# calculate the kl divergence KL(P || Q)\n",
    "def kl_divergence(p, q):\n",
    "\treturn sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    " \n",
    "# calculate entropy H(P)\n",
    "def entropy(p):\n",
    "\treturn -sum([p[i] * log2(p[i]) for i in range(len(p))])\n",
    " \n",
    "# calculate cross entropy H(P, Q)\n",
    "def cross_entropy(p, q):\n",
    "\treturn entropy(p) + kl_divergence(p, q)\n",
    " \n",
    "# define data\n",
    "p = [0.10, 0.40, 0.50]\n",
    "q = [0.80, 0.15, 0.05]\n",
    "# calculate H(P)\n",
    "en_p = entropy(p)\n",
    "print('H(P): %.3f bits' % en_p)\n",
    "# calculate kl divergence KL(P || Q)\n",
    "kl_pq = kl_divergence(p, q)\n",
    "print('KL(P || Q): %.3f bits' % kl_pq)\n",
    "# calculate cross entropy H(P, Q)\n",
    "ce_pq = cross_entropy(p, q)\n",
    "print('H(P, Q): %.3f bits' % ce_pq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
