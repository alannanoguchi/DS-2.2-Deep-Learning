{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning Glossary\n",
    "\n",
    "We will learn about deep learning components and terminologies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "\n",
    "- To allow Neural Networks to learn complex decision boundaries \n",
    "\n",
    "- we apply a nonlinear activation function to some of its layers\n",
    "\n",
    "- Commonly used functions include:\n",
    "\n",
    "    - sigmoid, tanh, ReLU (Rectified Linear Unit) and variants of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine x is our feature matrix, then \n",
    "\n",
    "what is x.shape[0]? - the number of samples\n",
    "\n",
    "what is x.shape[1]? - the number of features (columns) we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e26fd4b3c246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=x.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function (Cost Function)\n",
    "\n",
    "- When we build a neural network, the neural network tries to predict the output as close as possible to the actual value\n",
    "\n",
    "\n",
    "- For prediction type problem the cost functions are:\n",
    "\n",
    "    - MSE, MAE, ...\n",
    "    - `model.compile(optimizer='rmsprop', loss='mse')`   --> keras wants to minimize MSE\n",
    "    \n",
    "    \n",
    "- For classification type problem the cost functions are:\n",
    "\n",
    "    - Categorical Cross-Entropy, Binary Cross-Entropy\n",
    "    - `model.compile(optimizer='rmsprop', loss='binary_crossentropy')`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms (or Optimization Methods) to Minimize Error\n",
    "\n",
    "- Gradient Descent (GD): To think of it intuitively, while climbing down a hill you should take small steps and walk down instead of just jumping down at once. Therefore, what we do is, if we start from a point x, we move down a little i.e. delta h, and update our position to x-delta h and we keep doing the same till we reach the bottom\n",
    "\n",
    "\n",
    "- Stochastic gradient descent (SGD)\n",
    "\n",
    "\n",
    "- Learning rate: Both GD and SGD need learning rate to adjust the new weight \n",
    "\n",
    "    - w1_new= w1 - (learning rate)* (derivative of cost function wrt w1)\n",
    "    \n",
    "    - RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam\n",
    "    \n",
    "    - Check out book pdf Ch. 16 Optimizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "- Dropout is a regularization technique for Neural Networks that prevents overfitting\n",
    "\n",
    "- `model.add(Dropout(0.25))` --> removes 25% of connections from the bottom layer to the next layer\n",
    "\n",
    "<img src = 'https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/raw/master/Notebooks/Images/dropout.png' >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch and Batch\n",
    "\n",
    "- Epoch is when an ENTIRE dataset is passed forward and backward through the neural network\n",
    "\n",
    "    - `model.fit(x, y, epochs=10, validation_data=(x_val, y_val))`\n",
    "    \n",
    "- Batch is number of samples per gradient update\n",
    "\n",
    "    - `model.fit(x, y, batch_size=2, epochs=10)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply NN with Keras on iris data\n",
    "\n",
    "- Use 100 samples for training and 50 samples for validation\n",
    "\n",
    "- Set the value of epoch to 5 \n",
    "\n",
    "- Change the `batch_size` value from 1 to 100 and plot the accuracy versus batch_size\n",
    "\n",
    "- Change the `verbose` to 0, 1 and 2\n",
    "\n",
    "### Observations:\n",
    "1. Lower batch size better accuracy at the price of a slower computation\n",
    "\n",
    "2. The entire model should be in the for loop, if not then our starting points for the weighs and biases are at good condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-58-d39bd2d59f11>, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-58-d39bd2d59f11>\"\u001b[0;36m, line \u001b[0;32m38\u001b[0m\n\u001b[0;31m    accuracies.append(accuracy)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential \n",
    "from keras.layers.core import Dense, Activation\n",
    "\n",
    "# importing iris - 4 features in this dataset\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# split the data: train 70%, test 30%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "y_train_one_hot = np_utils.to_categorical(y_train)\n",
    "y_test_one_hot = np_utils.to_categorical(y_test)\n",
    "\n",
    "\n",
    "# instantiate Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# define one hidden layer with 16 neurons, input_shape = 4 for 4 features\n",
    "model.add(Dense(16, input_shape=(4,)))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "# 3 outputs for the 3 types of iris classes --> there should be 3 output neurons\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "model.fit(X_train, y_train_one_hot, epochs=100, batch_size=1, verbose=0);\n",
    "loss, accuracy = model.evaluate(X_test, y_test_one_hot, verbose=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity: Apply Lambda Layer in Keras and test how it works\n",
    "\n",
    "- Write a code that takes a array with size 3 and apply a Lambda Layer in Keras to double the arrays elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"la...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10. 24.  2.]]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from keras.layers import Lambda, Input\n",
    "from keras.models import Model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "input = Input(shape=(3,))\n",
    "double = Lambda(lambda x: 2 * x)(input)\n",
    "\n",
    "model = Model(input=input, output=double)\n",
    "model.compile(optimizer='sgd', loss='mse')\n",
    "\n",
    "data = np.array([[5, 12, 1]])    # 10, 24, 2\n",
    "print(model.predict(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "- Batch Normalization is a technique that normalizes the data even at hidden layers \n",
    "\n",
    "- `model.add(BatchNormalization())`\n",
    "\n",
    "<img src = 'https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/raw/master/Notebooks/Images/batch_normalization.png' width='600' height ='600'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is activation function? - allows NN to learn complex boundaries\n",
    "\n",
    "\n",
    "2. What is the loss function for two and three class, classification problem? - Binary cross-entropy is for multi-label classifications, whereas categorical cross entropy is for multi-class classification where each example belongs to a single class.\n",
    "\n",
    "\n",
    "3. What is optimizer in Keras? - It is the weight updating rule.\n",
    "\n",
    "\n",
    "4. What is dropout? - It reduces the number of connections to prevent overfitting. \n",
    "\n",
    "\n",
    "5. What is Batch Normalization? - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify images of handwritten digists (MNIST dataset) using a fully-connected NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt #This package is for plotting\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prepare Data:\n",
    "    \n",
    "    - the dataset is loaded in this section\n",
    "    \n",
    "1-0. Check the dimensions of data and its minimum and maximum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trian data dim: (60000, 28, 28)\n",
      "test data dim: (10000, 28, 28)\n",
      "test label dim: (10000,)\n",
      "max of trian data: 255\n",
      "min of train data: 0\n",
      "max of test data: 255\n",
      "min of test data: 0\n"
     ]
    }
   ],
   "source": [
    "print('trian data dim:', x_train.shape)\n",
    "print('test data dim:' , x_test.shape)\n",
    "print('test label dim:', y_test.shape)\n",
    "\n",
    "# Print the minimum and maximum of x_train and x_test(use numpy min and max functions)\n",
    "print('max of trian data:', np.max(x_train))\n",
    "print('min of train data:', np.min(x_train))\n",
    "print('max of test data:', np.max(x_test))\n",
    "print('min of test data:', np.min(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. Check a random sample of trian data and its label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 8\n"
     ]
    }
   ],
   "source": [
    "# generate a random number ( use numpy random.randint)\n",
    "rand_num = np.random.randint(60000)\n",
    "\n",
    "#plot using plt.imshow() and plt.show()\n",
    "plt.imshow(x_train[rand_num], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "#print its label\n",
    "print('label:', y_train[rand_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. Our network accept 1D data so we should flatten our 2D image, then print the dimension of the result arrays\n",
    "- use numpy reshape function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape x_train and x_test\n",
    "x_train = np.reshape(x_train,[-1, 28*28])\n",
    "x_test = np.reshape(x_test,[-1, 28*28])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. Normalize the data by rescaling them to (0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/np.max(x_train)\n",
    "x_test = x_test/np.max(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-4. Convert label arrays to 1-hot representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Model\n",
    "\n",
    "2-0. Add the following layers to the network:\n",
    "    - hidden layer 1: fully connected + Relu activation (e.g 512 neurons)\n",
    "    - hidden layer 2: fully connected + Relu activation (e.g 512 neurons)\n",
    "    - output layer: fully connected + Softamx activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # instantiate Sequential model\n",
    "# model = Sequential()\n",
    "\n",
    "# # define hidden layer 1 with 512 neurons, input_shape = 784 pixel image \n",
    "# model.add(Dense(512, input_shape=(784,)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "# # define hidden layer 2 with 512 neurons, input_shape = 784\n",
    "# model.add(Dense(512, input_shape=(784,)))\n",
    "# model.add(Activation('relu'))\n",
    "\n",
    "\n",
    "# # 10 outputs for the 10 types of ouput neurons --> there should be 10 output neurons\n",
    "# model.add(Dense(10))\n",
    "# model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))     # each image has 784 pixels because it is 28x28\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. Determine loss function, optimizer and metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=sgd,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Print the review of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_47 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_48 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_49 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 669,706\n",
      "Trainable params: 669,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "\n",
    "# here we saved the raw model without any training. we will use it later\n",
    "model.save('raw_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we have 401920 in the Param (this is the input to the first hidden layer)?\n",
    "- It is the number of weights from the input to first hidden layer (784*512) + 512 = 401920\n",
    "\n",
    "### Why do we have 262656?\n",
    "- It is the the number of weights from the first and second hidden layer (512*512) + 512 = 262656\n",
    "\n",
    "### Why do we have 5130?\n",
    "- This is the number of weights from the second hidden layer to the output (512*10) + 10 = 5130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train and Evaluate Model\n",
    "\n",
    "3-0. Train model on training data using model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/3\n",
      "48000/48000 [==============================] - 4s 91us/step - loss: 0.6363 - accuracy: 0.8465 - val_loss: 0.3215 - val_accuracy: 0.9084\n",
      "Epoch 2/3\n",
      "48000/48000 [==============================] - 4s 88us/step - loss: 0.2983 - accuracy: 0.9158 - val_loss: 0.2495 - val_accuracy: 0.9289\n",
      "Epoch 3/3\n",
      "48000/48000 [==============================] - 4s 92us/step - loss: 0.2466 - accuracy: 0.9295 - val_loss: 0.2173 - val_accuracy: 0.9395\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                   batch_size=32, \n",
    "                   epochs=3,\n",
    "                   verbose=1,\n",
    "                   validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-1. Evaluate model on test data using model.evaluate. Print the model accuracy on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.21809677972495556\n",
      "Test accuracy: 0.9388999938964844\n"
     ]
    }
   ],
   "source": [
    "te_score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "print('Test Loss:', te_score[0])\n",
    "print('Test accuracy:', te_score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-2. Save model \n",
    "\n",
    "In Keras, you can save the model to a HDF5 file(.h5) and reload it later simply by model.save(filepath) and keras.models.load_model(filepath), respectively.\n",
    "\n",
    "The saved model contains:\n",
    "* the architecture of the model, allowing to re-create the model\n",
    "* the weights of the model\n",
    "* the training configuration (loss, optimizer)\n",
    "* the state of the optimizer, allowing to resume training exactly where you left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model here:\n",
    "model.save('mlp.h5')\n",
    "# Delete model to make sure you reload it correctly:\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-3. Load model and Predict label for a random image in train set. Verify predicted label by ploting the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True label: 2\n",
      "Predicted label: 2\n"
     ]
    }
   ],
   "source": [
    "# reload the model here:\n",
    "model = load_model('mlp.h5')\n",
    "# generate a random number. (use numpy random.randint)\n",
    "rand_num = np.random.randint(60000)\n",
    "img = x_train[rand_num]\n",
    "img = np.expand_dims(img, axis = 0)\n",
    "predicted_label =model.predict(img, 1)\n",
    "img = np.reshape(img,(28,28))\n",
    "plt.imshow(img, cmap='gray' )\n",
    "plt.show()\n",
    "# print its label\n",
    "true_label = np.argmax(y_train[rand_num])\n",
    "predicted_label = np.argmax(predicted_label)\n",
    "print('True label:', true_label)\n",
    "print('Predicted label:', predicted_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we have classifcation problem, the ouput neurons is equal to the number of classes ( for example 3 for iris)\n",
    "\n",
    "### The activation function for output layer should be softmax and loss should be categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Always in classification problem: the np.sum(y_pred) = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
