{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a deep learning model on a large dataset\n",
    "\n",
    "- In Keras, using `fit()` and `predict()` is fine for smaller datasets which can be loaded into memory\n",
    "\n",
    "- But in practice, for most practical-use cases, almost all datasets are large and cannot be loaded into memory at once\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Objectives\n",
    "- Learn how to handle datasets that will not fit into memory and use them for Keras training\n",
    "- What is fit generator and predict generator\n",
    "- What is Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(1, 10)]    # a is an array\n",
    "\n",
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in a:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "b = (i for i in range(1, 10))      # b is a generator\n",
    "\n",
    "print(next(b))\n",
    "print(next(b))\n",
    "print(next(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass an interator (for example, b, we had above), then your generator returns the squared value for each element in b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "def f_generator(iterator):\n",
    "    for item in iterator:\n",
    "        yield item**2\n",
    "        \n",
    "b = (i for i in range(1, 10))\n",
    "G = f_generator(b)\n",
    "for i in G:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "\n",
    "- Assume we have a csv file with two columns. The first column is the url link of images and the second column is the image label \n",
    "\n",
    "- Assume the csv can fit into memory -> The following `df = pd.read_csv(' ')` is doable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def data_gen(df, batch_size):\n",
    "    while True:\n",
    "        x_batch = np.zeros((batch_size, 3, 224, 224))\n",
    "        ## Assume we have two class (two lables)\n",
    "        y_batch = np.zeros((batch_size, 1))\n",
    "        # the first loop goes through sections at a time\n",
    "        for j in range(len(df['url']/batch_size)):\n",
    "            b = 0\n",
    "            # inner loop reads the url values in each row of the section:\n",
    "            for m, k in zip(df['url'].values[j*batch_size:(j+1)*batch_size], df['class'].values[j*batch_size:(j+1)*batch_size]):\n",
    "                # see below for this step (we should read image url and have it as an image file, resize it, convert it to numpy array and scale it)\n",
    "                x_batch[b] = m\n",
    "                y_batch[b] = k\n",
    "                b += 1\n",
    "            yield (x_batch, y_batch)\n",
    "\n",
    "# use model.fit_generator not model.fit\n",
    "model.fit_generator(generator=data_gen(df_train, batch_size=batch_size), steps_per_epoch=len(df_train) // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detail implementation of reading image url, resize it, converting into numpy array with appropriate data size and scale it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib2\n",
    "from PIL import image\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "# in our example m is url\n",
    "request = urllib2.Request(url)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What if even that csv file cannot fit into memory?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df_chunk_train = pd.read_csv('*.csv', chunksize=10)\n",
    "\n",
    "C = 0\n",
    "for chunk in df_chunk_train:\n",
    "    C += 1\n",
    "\n",
    "def data_gen(df_chunk, batch_size):\n",
    "    while True:\n",
    "        for df in df_chunk:    # add another for loop and pass df_chunk into the function\n",
    "            x_batch = np.zeros((batch_size, 3, 224, 224))\n",
    "            ## Assume we have two class (two lables)\n",
    "            y_batch = np.zeros((batch_size, 1))\n",
    "            for j in range(len(df['url']/batch_size)):\n",
    "                b = 0\n",
    "                for m, k in zip(df['url'].values[j*batch_size:(j+1)*batch_size], df['class'].values[j*batch_size:(j+1)*batch_size]):\n",
    "                    x_batch[b] = m\n",
    "                    y_batch[b] = k\n",
    "                    b += 1\n",
    "                yield (x_batch, y_batch)\n",
    "\n",
    "\n",
    "model.fit_generator(generator=data_gen(df_chunk_train, batch_size=batch_size), steps_per_epoch=10*C // batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "- One of the best ways to improve the performance of a Deep Learning model is to add more variations of data to the training set\n",
    "    - ex) apply a small rotation on a hand-drawn number image\n",
    "\n",
    "\n",
    "- want the dataset to be representative of the many different positions, angles, lightings, and miscellaneous distortions\n",
    "\n",
    "\n",
    "\n",
    "- In keras there are two ways:\n",
    "\n",
    "    - Use `ImageDataGenerator`\n",
    "    - Write our custom code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,  #rotation\n",
    "        zoom_range=0.2,   #zoom\n",
    "        horizontal_flip=True)   #flip image\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        'data/train',\n",
    "        target_size=(150, 150),   #image size\n",
    "        batch_size=32,      # read in batches of 32 in a group\n",
    "        class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        'data/validation',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=32,\n",
    "        class_mode='binary')\n",
    "\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=2000,\n",
    "        epochs=50,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity: Do Data Augmentation for MNIST with Above Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import matplotlib.pyplot as plt #This package is for plotting\n",
    "%matplotlib inline  \n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Dense, Activation, Flatten, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 9\n"
     ]
    }
   ],
   "source": [
    "# generate a random number ( use numpy random.randint)\n",
    "rand_num = np.random.randint(60000)\n",
    "\n",
    "#plot using plt.imshow() and plt.show()\n",
    "plt.imshow(x_train[rand_num], cmap='gray')\n",
    "plt.show()\n",
    "\n",
    "#print its label\n",
    "print('label:', y_train[rand_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers.core import Activation\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units= 500, activation='relu'  ))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before using it.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-a68ee102f30c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m                     \u001b[0msteps_per_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m60\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                     epochs = 300)\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdo_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 303\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You must compile your model before using it.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_trainable_weights_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before using it."
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28,1)\n",
    "x_test = x_test.reshape(-1,28,28,1)\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=True,\n",
    "    featurewise_std_normalization=True,\n",
    "    rotation_range=10,\n",
    "    fill_mode='nearest',\n",
    "    validation_split = 0.2\n",
    "    )\n",
    "\n",
    "datagen.fit(x_train)\n",
    "\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size=60, subset='training')\n",
    "\n",
    "validation_generator = datagen.flow(x_train, y_train, batch_size=60, subset='validation')\n",
    "\n",
    "model.fit_generator(generator=train_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    steps_per_epoch = len(train_generator) / 60,\n",
    "                    validation_steps = len(validation_generator) / 60,\n",
    "                    epochs = 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/moghazy/guide-to-cnns-with-data-augmentation-keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning (TL)\n",
    "\n",
    "- In practice a very few people train a Convolution network from scratch (random initialisation) because it is rare to get enough dataset. So, using pre-trained network weights as initialisations or a fixed feature extractor helps in solving most of the problems in hand\n",
    "\n",
    "- Very Deep Networks are expensive to train. The most complex models take weeks to train using hundreds of machines equipped with expensive GPUs\n",
    "\n",
    "- Determining the topology/flavour/training method/hyper parameters for deep learning is a black art with not much theory to guide you.\n",
    "\n",
    "- So, we need transfer learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 input_6\n",
      "2 block1_conv1\n",
      "3 block1_conv2\n",
      "4 block1_pool\n",
      "5 block2_conv1\n",
      "6 block2_conv2\n",
      "7 block2_pool\n",
      "8 block3_conv1\n",
      "9 block3_conv2\n",
      "10 block3_conv3\n",
      "11 block3_pool\n",
      "12 block4_conv1\n",
      "13 block4_conv2\n",
      "14 block4_conv3\n",
      "15 block4_pool\n",
      "16 block5_conv1\n",
      "17 block5_conv2\n",
      "18 block5_conv3\n",
      "19 block5_pool\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "\n",
    "# with include_top=False we emphasize that only use CNN part of VGG16 and leaves out MLP\n",
    "base_model = applications.vgg16.VGG16(include_top=False, weights='imagenet')    # there are 16 convolutional layers\n",
    "\n",
    " \n",
    "i=0\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    i = i+1\n",
    "    print(i,layer.name)\n",
    "    \n",
    "# pooling does not have any learning parameters so block1_conv1, block1_conv2 etc have weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of convolutional layers\n",
    "2+2+3+3+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plus 3 MLP\n",
    "13+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When we print model.summary(), then # of non-trainable parameters would be equal to # of weights + biases in VGG16 and # of trainable parameters would be equal to the MLP part that we defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base_model.output\n",
    "x = Dense(128, activation='sigmoid')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.2)(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "\n",
    " \n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    " \n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(lr=0.001, momentum=0.9),metrics=[\"accuracy\"])\n",
    " \n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=100,\n",
    "        epochs=10,\n",
    "        callbacks = callbacks_list,\n",
    "        validation_data = validation_generator,\n",
    "        validation_steps=20\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
