{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are autoencoders?\n",
    "\n",
    "- In classical machine learning, we introduced PCA as a dimensionality reduction algorithm\n",
    "\n",
    "- In deep learning auto-encoder has the same purpose as PCA does for tabular data\n",
    "\n",
    "- \"Autoencoding\" is a data compression and decompression algorithm where the compression and decompression functions are implemented with neural networks\n",
    "\n",
    "- Two interesting practical applications of autoencoders are data denoising (which we feature later in this post), and dimensionality reduction\n",
    "\n",
    "<img src=\"https://github.com/Make-School-Courses/DS-2.2-Deep-Learning/raw/master/Notebooks/Images/auto_encoder.png\" width=\"500\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a MLP based auto-encoder for MNIST\n",
    "\n",
    "- Write a single fully-connected neural layer as encoder and as decoder with Keras\n",
    "\n",
    "### Steps:\n",
    "1. Reshape the data \n",
    "2. Encode 784 to 32\n",
    "3. Decode 32 to 784\n",
    "4. Use binary-crossentropy as loss function (per pixel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers import Input, Dense\n",
    "# from keras.models import Model\n",
    "# from keras.datasets import mnist\n",
    "# import numpy as np\n",
    "\n",
    "# input_img = Input(shape=(784,))\n",
    "\n",
    "# # encode the input image into 32\n",
    "# encode_layer = Dense(32, activation='relu')(input_img)\n",
    "\n",
    "# # decode the encoded layer into 784\n",
    "# decode_layer = Dense(784, activation='sigmoid')(encoded_layer)\n",
    "\n",
    "# model = Model(input_img, decode_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.3666 - val_loss: 0.2709\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2636 - val_loss: 0.2526\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2417 - val_loss: 0.2284\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2203 - val_loss: 0.2099\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2048 - val_loss: 0.1970\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1939 - val_loss: 0.1880\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1856 - val_loss: 0.1804\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1787 - val_loss: 0.1740\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1727 - val_loss: 0.1685\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1675 - val_loss: 0.1635\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1628 - val_loss: 0.1592\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.1585 - val_loss: 0.1549\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1546 - val_loss: 0.1512\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1510 - val_loss: 0.1479\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1478 - val_loss: 0.1447\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1448 - val_loss: 0.1417\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1421 - val_loss: 0.1392\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1396 - val_loss: 0.1367\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1373 - val_loss: 0.1345\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1352 - val_loss: 0.1325\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1332 - val_loss: 0.1306\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1313 - val_loss: 0.1287\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1295 - val_loss: 0.1270\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1278 - val_loss: 0.1252\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1262 - val_loss: 0.1236\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1246 - val_loss: 0.1222\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 2s 38us/step - loss: 0.1231 - val_loss: 0.1207\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1217 - val_loss: 0.1193\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1204 - val_loss: 0.1180\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1191 - val_loss: 0.1168\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1179 - val_loss: 0.1157\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1168 - val_loss: 0.1146\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1158 - val_loss: 0.1135\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1148 - val_loss: 0.1126\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1138 - val_loss: 0.1117\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1130 - val_loss: 0.1108\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1121 - val_loss: 0.1100\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1113 - val_loss: 0.1093\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1106 - val_loss: 0.1085\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1099 - val_loss: 0.1079\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1092 - val_loss: 0.1072\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 36us/step - loss: 0.1086 - val_loss: 0.1066\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1080 - val_loss: 0.1060\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1074 - val_loss: 0.1055\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1069 - val_loss: 0.1050\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1064 - val_loss: 0.1045\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1060 - val_loss: 0.1041\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1056 - val_loss: 0.1037\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1052 - val_loss: 0.1034\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 33us/step - loss: 0.1048 - val_loss: 0.1030\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd04c8d9c10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input: reduces input shape to 32\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input: expands the layer with 32 to 784\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# prepare our input data. We're using MNIST digits, and we're discarding the labels \n",
    "# since we're only interested in encoding/decoding the input images\n",
    "\n",
    "\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# Reshape x_train and x_test\n",
    "x_train = np.reshape(x_train,[-1, 28*28])\n",
    "x_test = np.reshape(x_test,[-1, 28*28])\n",
    "\n",
    "# train our autoencoder for 50 epochs\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want to visualize the reconstructed inputs and the encoded representations, what should we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution:\n",
    "\n",
    "# We should create a separate encoder model\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# As well as the decoder model\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer [-1] of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model by adding the encoded_input into the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# encode and decode some digits\n",
    "# note that we take them from the *test* set\n",
    "encoded_imgs = encoder.predict(x_test)\n",
    "decoded_imgs = decoder.predict(encoded_imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can have deep auto-encoder and convolutional auto-encoder\n",
    "\n",
    "https://blog.keras.io/building-autoencoders-in-keras.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity:\n",
    "\n",
    "- Train the simplest MLP auto-encoder\n",
    "- For 5 test images, plot the compressed image from the encoder with the size 8x4 and plot the decoded images for these 5 test images\n",
    "\n",
    "- Hint: Use below:\n",
    "\n",
    "    from PIL import Image\n",
    "    \n",
    "    import numpy as np\n",
    "    \n",
    "    img = Image.fromarray(data, 'RGB')\n",
    "    \n",
    "    img.save('my.png')\n",
    "    \n",
    "    img.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# this is the size of our encoded representations\n",
    "encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# this is our input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "# \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "# \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(784, activation='sigmoid')(encoded)\n",
    "\n",
    "# this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "\n",
    "# this model maps an input to its encoded representation\n",
    "encoder = Model(input_img, encoded)\n",
    "\n",
    "# create a placeholder for an encoded (32-dimensional) input\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "# retrieve the last layer of the autoencoder model\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "# create the decoder model\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "\n",
    "# First, we'll configure our model to use a per-pixel binary crossentropy loss, and the Adadelta optimizer:\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "# We will normalize all values between 0 and 1 and we will flatten the 28x28 images into vectors of size 784.\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.3587 - val_loss: 0.2707\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.2630 - val_loss: 0.2522\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.2432 - val_loss: 0.2319\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.2243 - val_loss: 0.2142\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 2s 32us/step - loss: 0.2087 - val_loss: 0.2003\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1966 - val_loss: 0.1901\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1876 - val_loss: 0.1823\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1807 - val_loss: 0.1762\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1750 - val_loss: 0.1710\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1702 - val_loss: 0.1665\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1658 - val_loss: 0.1623\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1617 - val_loss: 0.1584\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1578 - val_loss: 0.1546\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1541 - val_loss: 0.1511\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1506 - val_loss: 0.1475\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1473 - val_loss: 0.1443\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1442 - val_loss: 0.1413\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1413 - val_loss: 0.1384\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1386 - val_loss: 0.1359\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1361 - val_loss: 0.1333\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1337 - val_loss: 0.1310\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1314 - val_loss: 0.1288\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1293 - val_loss: 0.1267\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1272 - val_loss: 0.1247\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1253 - val_loss: 0.1229\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1235 - val_loss: 0.1211\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1218 - val_loss: 0.1194\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1202 - val_loss: 0.1179\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1187 - val_loss: 0.1164\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1173 - val_loss: 0.1150\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1160 - val_loss: 0.1138\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.1148 - val_loss: 0.1126\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1137 - val_loss: 0.1116\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1127 - val_loss: 0.1106\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1117 - val_loss: 0.1096\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 2s 34us/step - loss: 0.1108 - val_loss: 0.1088\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 2s 37us/step - loss: 0.1100 - val_loss: 0.1080\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.1092 - val_loss: 0.1072\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 3s 45us/step - loss: 0.1085 - val_loss: 0.1065\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 3s 42us/step - loss: 0.1079 - val_loss: 0.1059\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 2s 35us/step - loss: 0.1072 - val_loss: 0.1053\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1067 - val_loss: 0.1047\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 2s 29us/step - loss: 0.1061 - val_loss: 0.1042\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1056 - val_loss: 0.1037\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1051 - val_loss: 0.1033\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1047 - val_loss: 0.1028\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1043 - val_loss: 0.1024\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 2s 31us/step - loss: 0.1039 - val_loss: 0.1020\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 2s 30us/step - loss: 0.1035 - val_loss: 0.1016\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.1032 - val_loss: 0.1013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd0566cf850>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now let's train our autoencoder for 50 epochs:\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use Matplotlib (don't ask)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n = 5  # how many digits we will display\n",
    "plt.figure(figsize=(8, 4))\n",
    "for i in range(n):\n",
    "    # display original: top row\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction: bottom row\n",
    "    ax = plt.subplot(2, n, i + 1 + n)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the cost function to MSE or Keep the binary Cross Entropy \n",
    "\n",
    "- For AutoEncoder, we can not just simply do `model.evaluate` and check the accuracy. Maybe the `loss = mse` would be good. We can verify the model performance by human interaction (visually see the output of auto-encoder images) or feeding the output of AutoEncoder to a trained MLP or trained CNN to see how it works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
